{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440ea646-1284-4d6c-8632-23c35b75dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df0dac4-8fbb-41a6-ac9d-f2bcabd77efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"/mnt/hd1/ani/HIPT/HIPT_4K\")\n",
    "from hipt_4k import HIPT_4K\n",
    "from hipt_model_utils import get_vit256, get_vit4k, eval_transforms\n",
    "from hipt_heatmap_utils import *\n",
    "from attention_visualization_utils import *\n",
    "light_jet = cmap_map(lambda x: x/2 + 0.5, matplotlib.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf4ce21-5d3d-4f2f-a624-61b0ee14265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "_CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24251MB, multi_processor_count=64)\n",
      "Total memory in GB:  24.83328\n",
      "Memory reserved in GB:  0.0\n",
      "Memory allocated in GB:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Device + Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Check GPU properties\n",
    "for i in range(torch.cuda.device_count()): \n",
    "    print(torch.cuda.get_device_properties(i))\n",
    "    \n",
    "    # Get memory\n",
    "    print('Total memory in GB: ', torch.cuda.get_device_properties(i).total_memory/(1.024*1e9))\n",
    "    print('Memory reserved in GB: ', torch.cuda.memory_reserved(i)/(1.024*1e9))\n",
    "    print('Memory allocated in GB: ', torch.cuda.memory_allocated(i)/(1.024*1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ab0a14-b311-4103-8e4e-059aa1e0df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================\n",
       "Layer (type (var_name))                       Param #\n",
       "=========================================================\n",
       "HIPT_4K (HIPT_4K)                             --\n",
       "├─VisionTransformer (model256)                76,032\n",
       "│    └─PatchEmbed (patch_embed)               --\n",
       "│    │    └─Conv2d (proj)                     (295,296)\n",
       "│    └─Dropout (pos_drop)                     --\n",
       "│    └─ModuleList (blocks)                    --\n",
       "│    │    └─Block (0)                         (1,774,464)\n",
       "│    │    └─Block (1)                         (1,774,464)\n",
       "│    │    └─Block (2)                         (1,774,464)\n",
       "│    │    └─Block (3)                         (1,774,464)\n",
       "│    │    └─Block (4)                         (1,774,464)\n",
       "│    │    └─Block (5)                         (1,774,464)\n",
       "│    │    └─Block (6)                         (1,774,464)\n",
       "│    │    └─Block (7)                         (1,774,464)\n",
       "│    │    └─Block (8)                         (1,774,464)\n",
       "│    │    └─Block (9)                         (1,774,464)\n",
       "│    │    └─Block (10)                        (1,774,464)\n",
       "│    │    └─Block (11)                        (1,774,464)\n",
       "│    └─LayerNorm (norm)                       (768)\n",
       "│    └─Identity (head)                        --\n",
       "├─VisionTransformer4K (model4k)               38,016\n",
       "│    └─Sequential (phi)                       --\n",
       "│    │    └─Linear (0)                        (73,920)\n",
       "│    │    └─GELU (1)                          --\n",
       "│    │    └─Dropout (2)                       --\n",
       "│    └─Dropout (pos_drop)                     --\n",
       "│    └─ModuleList (blocks)                    --\n",
       "│    │    └─Block (0)                         (444,864)\n",
       "│    │    └─Block (1)                         (444,864)\n",
       "│    │    └─Block (2)                         (444,864)\n",
       "│    │    └─Block (3)                         (444,864)\n",
       "│    │    └─Block (4)                         (444,864)\n",
       "│    │    └─Block (5)                         (444,864)\n",
       "│    └─LayerNorm (norm)                       (384)\n",
       "│    └─Identity (head)                        --\n",
       "=========================================================\n",
       "Total params: 24,447,168\n",
       "Trainable params: 0\n",
       "Non-trainable params: 24,447,168\n",
       "========================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights256 = '../Checkpoints/vit256_small_dino.pth'\n",
    "pretrained_weights4k = '../Checkpoints/vit4k_xs_dino.pth'\n",
    "device256 = torch.device('cpu')\n",
    "device4k = torch.device('cpu')\n",
    "\n",
    "### ViT_256 + ViT_4K loaded independently (used for Attention Heatmaps)\n",
    "model256 = get_vit256(pretrained_weights=pretrained_weights256, device=device256)\n",
    "model4k = get_vit4k(pretrained_weights=pretrained_weights4k, device=device4k)\n",
    "\n",
    "### ViT_256 + ViT_4K loaded into HIPT_4K API\n",
    "model = HIPT_4K(pretrained_weights256, pretrained_weights4k, device256, device4k)\n",
    "model.eval()\n",
    "\n",
    "# Print model summary\n",
    "summary(model=model,\n",
    "        col_width=12,\n",
    "        row_settings=[\"var_names\"]\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b7028-bc82-4201-8d7d-eef1de353152",
   "metadata": {},
   "source": [
    "## Standalone HIPT_4K Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbe43d3-a1a9-4acc-989e-caeb76beb7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 3, 4096, 4096])\n",
      "Output Shape: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "region = Image.open('/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/Testing_sd_30/Test_set_NASH_id_105_ACR_id_504/NASH/105_14_4.png')\n",
    "region = region.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "\n",
    "x = eval_transforms()(region).unsqueeze(dim=0)\n",
    "print('Input Shape:', x.shape)\n",
    "print('Output Shape:', model.forward(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe2d63-5c5f-4aa5-806f-eb6755537043",
   "metadata": {},
   "source": [
    "## HIPT_4K Attention Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8907f80-a064-4e42-b11e-c96f9a7f5825",
   "metadata": {},
   "source": [
    "##### Code for producing attention results (for [256 x 256], [4096 x 4096], and hierarchical [4096 x 4096]) can be run (as-is) below. There are several ways these results can be run:\n",
    "\n",
    "hipt_4k.py Class (Preferred): This class blends inference and heatmap creation in a seamless and more object-oriented manner, and is where I am focusing my future code development around.\n",
    "Helper Functions in hipt_heatmap_utils.py (Soon-to-be-deprecated): Heatmap creation was originally written as helper functions. May be more useful and easier from research perspective.\n",
    "Please use whatever is most helpful for your use case :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8df08-15ef-4275-8660-7dd2bd96ec28",
   "metadata": {},
   "source": [
    "## 256 x 256 Demo (Saving Attention Maps Individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d14bafa-f2c3-4447-84ac-c6eadf1d4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Lets automate this to save Attention maps of a bunch of images we wish to analyze\n",
    "# path_test_imgs = '/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_256x256/Testing_sd_30/Test_set_NASH_id_156_ACR_id_262/NASH/'\n",
    "# output_dir_parent = '/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/4096x4096 images/Test_set_NASH_id_156/256_individual/'\n",
    "\n",
    "# test_imgs_list = (glob.glob(path_test_imgs + \"*.png\"))\n",
    "# # len(test_imgs_list)\n",
    "\n",
    "# for i in range(len(test_imgs_list)):\n",
    "#     patch = Image.open(test_imgs_list[i])\n",
    "#     patch = patch.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "#     output_dir = output_dir_parent + str(i+1)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     create_patch_heatmaps_indiv(patch, model256, \n",
    "#                                    output_dir, fname=('patch'+str(i+1)), \n",
    "#                                    cmap=plt.get_cmap('coolwarm'), device256=torch.device(\"cuda\")\n",
    "#                                    )\n",
    "#     del patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0de90-5bcc-4d35-814a-3194aaca74aa",
   "metadata": {},
   "source": [
    "## 256 x 256 Demo (Concatenating + Saving Attention Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c76b6da-4f75-47ab-9058-fd41dce2bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Lets automate this to save Attention maps of a bunch of images we wish to analyze\n",
    "# path_test_imgs = '/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_256x256/Testing_sd_30/Test_set_NASH_id_156_ACR_id_262/NASH/'\n",
    "# output_dir_parent = '/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/4096x4096 images/Test_set_NASH_id_156/256_concat/'\n",
    "\n",
    "# test_imgs_list = (glob.glob(path_test_imgs + \"*.png\"))\n",
    "# # len(test_imgs_list)\n",
    "\n",
    "# for i in range(len(test_imgs_list)):\n",
    "#     patch = Image.open(test_imgs_list[i])\n",
    "#     patch = patch.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "#     output_dir = output_dir_parent + str(i+1)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     create_patch_heatmaps_concat(patch, model256, \n",
    "#                                    output_dir, fname=('patch'+str(i+1)), \n",
    "#                                    cmap=plt.get_cmap('coolwarm'), alpha=0.5,\n",
    "#                                    )\n",
    "#     del patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e56dd-a036-4f1b-84cb-9efadfafbd99",
   "metadata": {},
   "source": [
    "## 4096 x 4096 Demo (Saving Attention Maps Individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da7632a6-9347-4fc4-8f8b-3b71c6212540",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets automate this to save Attention maps of a bunch of images we wish to analyze\n",
    "path_test_imgs = '/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/Patient_image_sets_sd_30/104N/'\n",
    "output_dir_parent = '/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/4096x4096_images/Anh_slides/'\n",
    "\n",
    "test_imgs_list = (glob.glob(path_test_imgs + \"*.png\"))\n",
    "# len(test_imgs_list)\n",
    "\n",
    "for i in range(1):\n",
    "    region = Image.open(test_imgs_list[i])\n",
    "    region = region.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "    output_dir = output_dir_parent + str(i+1)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    create_hierarchical_heatmaps_indiv(region, model256, model4k, \n",
    "                                   output_dir, fname='region', \n",
    "                                   scale=2, threshold=0.5, cmap=light_jet, alpha=0.5)\n",
    "    del region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d6096",
   "metadata": {},
   "source": [
    "### We want to generate Attention maps for all 4096x4096 regions extracted from \"NASH\" + \"ACR\" WSIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b108eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets automate this to save Attention maps of a bunch of images we wish to analyze\n",
    "path_test_imgs = '/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/Anh_slides/'\n",
    "output_dir_parent = '/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/4096x4096_images/'\n",
    "\n",
    "labels = [\"ACR\"]\n",
    "\n",
    "for l in range(len(labels)):\n",
    "\n",
    "    # Check of \"NASH\"/\"ACR\" folders exist\n",
    "    if (os.path.exists(path_test_imgs + labels[l]) == True):   # check for NASH folder\n",
    "    \n",
    "        # Get list of patient folders\n",
    "        indiv_patient_dirs = [f.path for f in os.scandir(path_test_imgs + labels[l]) if f.is_dir()]\n",
    "        # print(indiv_patient_dirs)\n",
    "\n",
    "        for i in range(1, len(indiv_patient_dirs)):   # for each patient\n",
    "        \n",
    "            # if i == 3:\n",
    "            #     break\n",
    "\n",
    "            # For each 4096x4096 region, generate multi-head Attention maps\n",
    "            imgs_4k_list = (glob.glob(indiv_patient_dirs[i] + \"/\" + \"*.png\"))\n",
    "\n",
    "            for j in range(len(imgs_4k_list)):   # for each 4k image\n",
    "                    \n",
    "                region = Image.open(imgs_4k_list[j])\n",
    "                region = region.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "                output_dir = output_dir_parent + labels[l] + \"/\" + os.path.splitext(os.path.basename(indiv_patient_dirs[i]))[0] + \"/\" + str(j+1)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                create_hierarchical_heatmaps_indiv(region, model256, model4k, output_dir, \n",
    "                                               fname='region', scale=2, threshold=0.5, cmap=light_jet, alpha=0.5)\n",
    "                del region    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eaa3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([1, 3, 4096, 4096])\n",
      "(256, 256, 256, 3)\n",
      "(256, 6, 256, 256)\n",
      "(6, 4096, 4096)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf47ce1ab0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# region = Image.open(imgs_4k_list[-1])\n",
    "# region = region.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "# x = eval_transforms()(region).unsqueeze(dim=0)\n",
    "# print(\"Input Shape: \", x.shape)\n",
    "\n",
    "# b256, a256, a4k = get_region_attention_scores(region, model256, model4k, scale=1)\n",
    "\n",
    "# print(b256.shape)\n",
    "# print(a256.shape)\n",
    "# print(a4k.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40431b84-9141-4903-9617-3357bc34b199",
   "metadata": {},
   "source": [
    "## 4096 x 4096 Demo (Concatenating + Saving Attention Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9c03d7-7d26-4d2a-8c8f-612d66aba00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Lets automate this to save Attention maps of a bunch of images we wish to analyze\n",
    "# path_test_imgs = '/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/Testing_sd_30/Test_set_NASH_id_156_ACR_id_262/NASH/'\n",
    "# output_dir_parent = '/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/4096x4096 images/Test_set_NASH_id_156/4k_concat/'\n",
    "\n",
    "# test_imgs_list = (glob.glob(path_test_imgs + \"*.png\"))\n",
    "# # len(test_imgs_list)\n",
    "\n",
    "# for i in range(len(test_imgs_list)):\n",
    "#     region = Image.open(test_imgs_list[i])\n",
    "#     region = region.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)    \n",
    "#     output_dir = output_dir_parent + str(i+1)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     create_hierarchical_heatmaps_concat(region, model256, model4k, \n",
    "#                                    output_dir, fname=('region'+str(i+1)), \n",
    "#                                    scale=2, cmap=plt.get_cmap('coolwarm'), alpha=0.5,\n",
    "#                                    )\n",
    "#     del region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf20725-5ea4-4218-83b5-4b599ac58242",
   "metadata": {},
   "source": [
    "## Preparing dataset for slide-level classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ed578-6594-4b33-b1cb-40bfdce8a816",
   "metadata": {},
   "source": [
    "#### Basically, we want to analyze all 4K tiles: for each slide, get learned feature vectors for each tile and take average to obtain slide-level representation. If \"NASH\", label=1, if \"ACR\", label=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e87c48-5d99-462f-840f-52eafb79e54c",
   "metadata": {},
   "source": [
    "#### Creating \"NASH\", \"ACR\" image folders with patient sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32d3c18-a2bb-4f34-9a39-48c455277604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_imgs = \"/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/Patient_image_sets_sd_30/\"\n",
    "\n",
    "# labels = [\"NASH\", \"ACR\"]\n",
    "\n",
    "# ### Let us dump 4096x4096 regions of \"ACR\" and \"NASH\" into 2 separate folders\n",
    "# indiv_patient_imgs = [f.path for f in os.scandir(path_imgs) if f.is_dir()]\n",
    "# unwanted_files = glob.glob(path_imgs + \".*ipynb_checkpoints\")\n",
    "# # indiv_patient_imgs\n",
    "\n",
    "# # Create \"NASH\" and \"ACR folders\"\n",
    "# path_folders = (\"/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/\")\n",
    "# if (os.path.exists(path_folders + labels[1]) == False) & (os.path.exists(path_folders + labels[0]) == False):\n",
    "#     os.mkdir(path_folders + labels[1])   # creates ACR folder\n",
    "#     os.mkdir(path_folders + labels[0])   # creates NASH folder\n",
    "\n",
    "# for i in range(len(indiv_patient_imgs)):\n",
    "#     if indiv_patient_imgs[i] == unwanted_files[0]:\n",
    "#         continue\n",
    "    \n",
    "#     if indiv_patient_imgs[i][-1] == 'A':\n",
    "#         res_level_folder_a = [g.path for g in os.scandir(indiv_patient_imgs[i]) if g.is_dir()]\n",
    "#         high_sig_folder_a = [h.path for h in os.scandir(res_level_folder_a[0]) if h.is_dir()][1]\n",
    "        \n",
    "#         # Get list of \"png\" files\n",
    "#         img_list = (glob.glob(high_sig_folder_a + \"/\" + \"*.png\"))\n",
    "        \n",
    "#         # Copy files to \"ACR\" folder but put in specific patient sub-folder\n",
    "#         path_acr_copy_dest = path_folders + labels[1] + \"/\" + os.path.splitext(os.path.basename(indiv_patient_imgs[i]))[0][:-1]\n",
    "#         if os.path.exists(path_acr_copy_dest) == False:\n",
    "#             os.mkdir(path_acr_copy_dest)\n",
    "#             [shutil.copy(img_list[j], path_acr_copy_dest) for j in range(len(img_list))]\n",
    "        \n",
    "#     else:\n",
    "#         res_level_folder_n = [g.path for g in os.scandir(indiv_patient_imgs[i]) if g.is_dir()]\n",
    "#         high_sig_folder_n = [h.path for h in os.scandir(res_level_folder_n[0]) if h.is_dir()][1]\n",
    "        \n",
    "#         # Get list of \"png\" files\n",
    "#         img_list = (glob.glob(high_sig_folder_n + \"/\" + \"*.png\"))\n",
    "        \n",
    "#         # Copy files to \"NASH\" folder but put in specific patient sub-folder\n",
    "#         path_nash_copy_dest = path_folders + labels[0] + \"/\" + os.path.splitext(os.path.basename(indiv_patient_imgs[i]))[0][:-1]\n",
    "#         if os.path.exists(path_nash_copy_dest) == False:\n",
    "#             os.mkdir(path_nash_copy_dest)\n",
    "#             [shutil.copy(img_list[l], path_nash_copy_dest) for l in range(len(img_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29201eff-4f1d-4d10-add2-079786e6916f",
   "metadata": {},
   "source": [
    "### Create data matrix with (#patients, #features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab85a5eb-e1ae-48e3-b4c7-1845bb69d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First lets get the patient sub-folders\n",
    "# path_folders = (\"/mnt/hd1/ani/Liver_pathology_project/Prepared_datasets/Tile_size_4096x4096/\")\n",
    "# labels = [\"NASH\", \"ACR\"]\n",
    "# indiv_patient_folders_n = [p.path for p in os.scandir(path_folders + labels[0]) if p.is_dir()]\n",
    "# indiv_patient_folders_a = [q.path for q in os.scandir(path_folders + labels[1]) if q.is_dir()]\n",
    "\n",
    "# n_out_features = 192   # HIPT feature dimension\n",
    "\n",
    "# feat_tensor_n, feat_tensor_a = torch.zeros(len(indiv_patient_folders_n), n_out_features), torch.zeros(len(indiv_patient_folders_a), n_out_features) \n",
    "\n",
    "# # NASH dataset\n",
    "# for i in range(len(indiv_patient_folders_n)):\n",
    "#     # Path of NASH images\n",
    "#     imgs_list_nash = (glob.glob(indiv_patient_folders_n[i] + \"/\" + \"*.png\"))\n",
    "    \n",
    "#     # Initializing tensors to store extracted features\n",
    "#     feat_tensor_nash = torch.zeros(len(imgs_list_nash), n_out_features)\n",
    "    \n",
    "#     for j in range(len(imgs_list_nash)):\n",
    "#         region_nash = Image.open(imgs_list_nash[j])\n",
    "#         region_nash = region_nash.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "\n",
    "#         # Forward propagate to get features\n",
    "#         feat_tensor_nash[j,:] = hipt_forward_pass(region_nash, model256, model4k, scale=1, \n",
    "#                                                        device256=torch.device('cuda:0'), \n",
    "#                                                        device4k=torch.device('cuda:0'))\n",
    "        \n",
    "#     # Compute mean embedding as slide-level representation\n",
    "#     feat_tensor_n[i, :] = torch.mean(feat_tensor_nash, dim=0)\n",
    "                \n",
    "        \n",
    "# # ACR dataset      \n",
    "# for u in range(len(indiv_patient_folders_a)):\n",
    "#     # Path of ACR images\n",
    "#     imgs_list_acr = (glob.glob(indiv_patient_folders_a[u] + \"/\" + \"*.png\"))\n",
    "    \n",
    "#     # Initializing tensors to store extracted features\n",
    "#     feat_tensor_acr = torch.zeros(len(imgs_list_acr), n_out_features)\n",
    "    \n",
    "#     for v in range(len(imgs_list_acr)):\n",
    "#         region_acr = Image.open(imgs_list_acr[v])\n",
    "#         region_acr = region_acr.convert('RGB')   # converting MxNx4 (RGBA) --> MxNx3 (RGB format)\n",
    "\n",
    "#         # Forward propagate to get features\n",
    "#         feat_tensor_acr[v,:] = hipt_forward_pass(region_acr, model256, model4k, scale=1, \n",
    "#                                                        device256=torch.device('cuda:0'), \n",
    "#                                                        device4k=torch.device('cuda:0'))\n",
    "        \n",
    "#     # Compute mean embedding as slide-level representation\n",
    "#     feat_tensor_a[u, :] = torch.mean(feat_tensor_acr, dim=0)\n",
    "\n",
    "    \n",
    "# # Lets also create tensors to store labels: \"1\" for Recurrent and \"0\" for Primary\n",
    "# labels_n, labels_a = torch.ones(len(indiv_patient_folders_n),), torch.zeros(len(indiv_patient_folders_a),)\n",
    "\n",
    "# # Checking tensor shapes\n",
    "# print('Shape of NASH feature tensor: ', feat_tensor_n.shape)\n",
    "# print('Shape of ACR feature tensor: ', feat_tensor_a.shape)\n",
    "# print('Shape of NASH label tensor: ', labels_n.shape)\n",
    "# print('Shape of ACR label tensor: ', labels_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0b3b08-05de-4b42-a1ac-d4f68cc7b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting all tensors to NumPy\n",
    "# feat_tensor_n_np, feat_tensor_a_np, labels_n_np, labels_a_np = feat_tensor_n.numpy(), feat_tensor_a.numpy(), labels_n.numpy(), labels_a.numpy()\n",
    "\n",
    "# # Keep equal no. of samples for both classes\n",
    "# num_samples = np.minimum(len(indiv_patient_folders_n), len(indiv_patient_folders_a))\n",
    "\n",
    "# # Randomly choose samples - this will be important for the class which has higher #examples\n",
    "# inds = np.random.randint(num_samples, size=num_samples)\n",
    "\n",
    "# feat_tensor_n_np_s, feat_tensor_a_np_s = feat_tensor_n_np[inds, :], feat_tensor_a_np[inds, :]\n",
    "# labels_n_np_s, labels_a_np_s = labels_n_np[inds], labels_a_np[inds]\n",
    "\n",
    "# # Stack class tensors\n",
    "# features = np.vstack((feat_tensor_n_np_s, feat_tensor_a_np_s))\n",
    "# labels = np.hstack((labels_n_np_s, labels_a_np_s))\n",
    "\n",
    "# # print('Feature dimension: ', features.shape)\n",
    "# # print('Label dimension: ', labels.shape)\n",
    "\n",
    "# data = np.zeros((features.shape[0], features.shape[1]+1))\n",
    "# data[:, :-1] = features\n",
    "# data[:, -1] = labels\n",
    "\n",
    "# print('Dataset dimension: ', data.shape)\n",
    "\n",
    "# ### Saving data as \".csv\"\n",
    "# df_data = pd.DataFrame(data)\n",
    "# df_data.to_csv(\"/mnt/hd1/ani/Liver_pathology_project/Data/HIPT_dataset_4kfeatures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635021c3",
   "metadata": {},
   "source": [
    "## Train traditional ML models and evaluate performance: repeated sampling for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6a51fb-093c-4627-917f-0c0b76ec4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RF, batch:1, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 1.0\n",
      "Train AUC: 1.0\n",
      "Test AUC: 1.0\n",
      "\n",
      "Training RF, batch:2, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 0.876984126984127\n",
      "Train AUC: 1.0\n",
      "Test AUC: 0.9\n",
      "\n",
      "Training RF, batch:3, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 1.0\n",
      "Train AUC: 1.0\n",
      "Test AUC: 1.0\n",
      "\n",
      "Training RF, batch:4, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 1.0\n",
      "Train AUC: 1.0\n",
      "Test AUC: 1.0\n",
      "\n",
      "Training RF, batch:5, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 0.4246031746031746\n",
      "Train AUC: 1.0\n",
      "Test AUC: 0.6428571428571428\n",
      "\n",
      "Training RF, batch:6, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 0.873015873015873\n",
      "Train AUC: 1.0\n",
      "Test AUC: 0.875\n",
      "\n",
      "Training RF, batch:7, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 0.873015873015873\n",
      "Train AUC: 1.0\n",
      "Test AUC: 0.875\n",
      "\n",
      "Training RF, batch:8, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 1.0\n",
      "Train AUC: 1.0\n",
      "Test AUC: 1.0\n",
      "\n",
      "Training RF, batch:9, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 0.876984126984127\n",
      "Train AUC: 1.0\n",
      "Test AUC: 0.9\n",
      "\n",
      "Training RF, batch:10, n_estimators=100, max_features=3, min_sample_leaf=2\n",
      "Train F1-score: 1.0\n",
      "Test F1-score: 1.0\n",
      "Train AUC: 1.0\n",
      "Test AUC: 1.0\n",
      "\n",
      "Final train F1-score mean:  1.0\n",
      "Final train F1-score std:  0.0\n",
      "Final test F1-score mean:  0.8924603174603174\n",
      "Final test F1-score std:  0.16671815153394196\n",
      "Final train AUC mean:  1.0\n",
      "Final train AUC std:  0.0\n",
      "Final test AUC mean:  0.9192857142857143\n",
      "Final test AUC std:  0.10660818975317507\n"
     ]
    }
   ],
   "source": [
    "# Read data from \".csv\" file\n",
    "dataset = pd.read_csv(\"/mnt/hd1/ani/Liver_pathology_project/Data/HIPT_dataset_4kfeatures.csv\", index_col=0)\n",
    "dataset = dataset.to_numpy()\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "\n",
    "out_file = \"/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/rf_hparams.txt\"\n",
    "\n",
    "num_iters = 10\n",
    "f1_train, f1_test, auc_train, auc_test = [], [], [], []\n",
    "\n",
    "f1_score_train, f1_score_test, auc_val_train, auc_val_test = np.zeros((num_iters,)), np.zeros((num_iters,)), np.zeros((num_iters,)), np.zeros((num_iters,))\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True)\n",
    "    \n",
    "    # Random Forest\n",
    "    for n_estimators in (50, 100):\n",
    "        for max_features in (2, 3):\n",
    "            for min_samples_leaf in (1, 2):\n",
    "                with open(out_file, \"a\") as f:\n",
    "                    # f.write(f\"MODEL: batch:{i+1}, n_estimators={n_estimators}, max_features={max_features}, min_sample_leaf={min_samples_leaf}\\n\")\n",
    "                    # f.write(\"\" +'\\n')\n",
    "                    # print(f\"Training RF, batch:{i+1}, n_estimators={n_estimators}, max_features={max_features}, min_sample_leaf={min_samples_leaf}\")\n",
    "                    \n",
    "                    model = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                                   max_features=max_features, \n",
    "                                                   min_samples_leaf=min_samples_leaf, \n",
    "                                                   random_state=1,\n",
    "                                                   n_jobs=-1)\n",
    "                    # Training\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Get predictions on train + test set\n",
    "                    y_preds_train = model.predict(X_train)\n",
    "                    y_preds_test = model.predict(X_test)\n",
    "                    \n",
    "                    # Evaluate F-score on train + test set\n",
    "                    f1_train.append(f1_score(y_train, y_preds_train, average='weighted'))\n",
    "                    f1_test.append(f1_score(y_test, y_preds_test, average='weighted'))\n",
    "                    \n",
    "                    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_preds_train, pos_label=1)\n",
    "                    auc_train.append(auc(fpr_train, tpr_train))\n",
    "                    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_preds_test, pos_label=1)\n",
    "                    auc_test.append(auc(fpr_test, tpr_test))\n",
    "                    \n",
    "                    # for each iteration, we want to store results only for the best set of hyperparams\n",
    "                    if f1_train[-1] == np.max(f1_train):\n",
    "                        n_est, max_feat, min_samp_leaf = n_estimators, max_features, min_samples_leaf\n",
    "                        f1_train_f, f1_test_f, auc_train_f, auc_test_f = f1_train[-1], f1_test[-1], auc_train[-1], auc_test[-1]\n",
    "\n",
    "    f1_train, f1_test = [], []\n",
    "                                    \n",
    "    print(f\"Training RF, batch:{i+1}, n_estimators={n_est}, max_features={max_feat}, min_sample_leaf={min_samp_leaf}\")\n",
    "    print(\"Train F1-score: \" + str(f1_train_f) + \"\\nTest F1-score: \" + str(f1_test_f) + \"\\nTrain AUC: \" + str(auc_train_f) + \"\\nTest AUC: \" + str(auc_test_f)) \n",
    "    print('')\n",
    "    \n",
    "    # try:\n",
    "    #     with open(out_file, \"a\") as f:\n",
    "    #         f.write(\"Training F1-score: \" + str(f1_train_f)+'\\n')\n",
    "    #         f.write(\"Testing F1-score: \" + str(f1_test_f)+'\\n')\n",
    "    # except ValueError:\n",
    "    #     with open(out_file, \"a\") as f:\n",
    "    #         f.write(\"NaN\\n\")\n",
    "            \n",
    "    f1_score_train[i], f1_score_test[i], auc_val_train[i], auc_val_test[i] = f1_train_f, f1_test_f, auc_train_f, auc_test_f\n",
    "\n",
    "print(\"Final train F1-score mean: \", np.mean(f1_score_train))\n",
    "print(\"Final train F1-score std: \", np.std(f1_score_train))\n",
    "print(\"Final test F1-score mean: \", np.mean(f1_score_test))\n",
    "print(\"Final test F1-score std: \", np.std(f1_score_test))\n",
    "print(\"Final train AUC mean: \", np.mean(auc_val_train))\n",
    "print(\"Final train AUC std: \", np.std(auc_val_train))\n",
    "print(\"Final test AUC mean: \", np.mean(auc_val_test))\n",
    "print(\"Final test AUC std: \", np.std(auc_val_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c7c8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "classes = (\"ACR\", \"NASH\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_preds_test)\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in classes], columns = [i for i in classes])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize = (5, 4))\n",
    "sns.heatmap(df_cm, annot=True, fmt=\".0f\", cbar=False);\n",
    "# plt.savefig(f\"/mnt/hd1/ani/Liver_pathology_project/Results/HIPT/ConfMat_test_frac_0p7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6baf879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a2edfd9441314e81940a8db140f035b2781395eeff1de1e2eee74d8a060d612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
