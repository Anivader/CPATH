{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440ea646-1284-4d6c-8632-23c35b75dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df0dac4-8fbb-41a6-ac9d-f2bcabd77efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"/mnt/hd1/ani/HIPT/HIPT_4K\")\n",
    "from hipt_4k import HIPT_4K\n",
    "from hipt_model_utils import get_vit256, get_vit4k, eval_transforms\n",
    "from hipt_heatmap_utils import *\n",
    "from attention_visualization_utils import *\n",
    "light_jet = cmap_map(lambda x: x/2 + 0.5, matplotlib.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf4ce21-5d3d-4f2f-a624-61b0ee14265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "_CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24251MB, multi_processor_count=64)\n",
      "Total memory in GB:  24.83328\n",
      "Memory reserved in GB:  0.0\n",
      "Memory allocated in GB:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Device + Hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Check GPU properties\n",
    "for i in range(torch.cuda.device_count()): \n",
    "    print(torch.cuda.get_device_properties(i))\n",
    "    \n",
    "    # Get memory\n",
    "    print('Total memory in GB: ', torch.cuda.get_device_properties(i).total_memory/(1.024*1e9))\n",
    "    print('Memory reserved in GB: ', torch.cuda.memory_reserved(i)/(1.024*1e9))\n",
    "    print('Memory allocated in GB: ', torch.cuda.memory_allocated(i)/(1.024*1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ab0a14-b311-4103-8e4e-059aa1e0df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit256_small_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "# of Patches: 196\n",
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./Checkpoints/vit4k_xs_dino.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================\n",
       "Layer (type (var_name))                       Param #\n",
       "=========================================================\n",
       "HIPT_4K (HIPT_4K)                             --\n",
       "├─VisionTransformer (model256)                76,032\n",
       "│    └─PatchEmbed (patch_embed)               --\n",
       "│    │    └─Conv2d (proj)                     (295,296)\n",
       "│    └─Dropout (pos_drop)                     --\n",
       "│    └─ModuleList (blocks)                    --\n",
       "│    │    └─Block (0)                         (1,774,464)\n",
       "│    │    └─Block (1)                         (1,774,464)\n",
       "│    │    └─Block (2)                         (1,774,464)\n",
       "│    │    └─Block (3)                         (1,774,464)\n",
       "│    │    └─Block (4)                         (1,774,464)\n",
       "│    │    └─Block (5)                         (1,774,464)\n",
       "│    │    └─Block (6)                         (1,774,464)\n",
       "│    │    └─Block (7)                         (1,774,464)\n",
       "│    │    └─Block (8)                         (1,774,464)\n",
       "│    │    └─Block (9)                         (1,774,464)\n",
       "│    │    └─Block (10)                        (1,774,464)\n",
       "│    │    └─Block (11)                        (1,774,464)\n",
       "│    └─LayerNorm (norm)                       (768)\n",
       "│    └─Identity (head)                        --\n",
       "├─VisionTransformer4K (model4k)               38,016\n",
       "│    └─Sequential (phi)                       --\n",
       "│    │    └─Linear (0)                        (73,920)\n",
       "│    │    └─GELU (1)                          --\n",
       "│    │    └─Dropout (2)                       --\n",
       "│    └─Dropout (pos_drop)                     --\n",
       "│    └─ModuleList (blocks)                    --\n",
       "│    │    └─Block (0)                         (444,864)\n",
       "│    │    └─Block (1)                         (444,864)\n",
       "│    │    └─Block (2)                         (444,864)\n",
       "│    │    └─Block (3)                         (444,864)\n",
       "│    │    └─Block (4)                         (444,864)\n",
       "│    │    └─Block (5)                         (444,864)\n",
       "│    └─LayerNorm (norm)                       (384)\n",
       "│    └─Identity (head)                        --\n",
       "=========================================================\n",
       "Total params: 24,447,168\n",
       "Trainable params: 0\n",
       "Non-trainable params: 24,447,168\n",
       "========================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights256 = './Checkpoints/vit256_small_dino.pth'\n",
    "pretrained_weights4k = './Checkpoints/vit4k_xs_dino.pth'\n",
    "device256 = torch.device(\"cpu\")\n",
    "device4k = torch.device(\"cpu\")\n",
    "\n",
    "### ViT_256 + ViT_4K loaded independently (used for Attention Heatmaps)\n",
    "model256 = get_vit256(pretrained_weights=pretrained_weights256, device=device256)\n",
    "model4k = get_vit4k(pretrained_weights=pretrained_weights4k, device=device4k)\n",
    "\n",
    "### ViT_256 + ViT_4K loaded into HIPT_4K API\n",
    "model = HIPT_4K(pretrained_weights256, pretrained_weights4k, device256, device4k)\n",
    "model.eval()\n",
    "\n",
    "# Print model summary\n",
    "summary(model=model,\n",
    "        col_width=12,\n",
    "        row_settings=[\"var_names\"]\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b7028-bc82-4201-8d7d-eef1de353152",
   "metadata": {},
   "source": [
    "## Standalone HIPT_4K Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbe43d3-a1a9-4acc-989e-caeb76beb7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 3, 4096, 4096])\n",
      "Output Shape: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "region = Image.open('./image_demo/image_4k.png')\n",
    "x = eval_transforms()(region).unsqueeze(dim=0)\n",
    "print('Input Shape:', x.shape)\n",
    "print('Output Shape:', model.forward(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe2d63-5c5f-4aa5-806f-eb6755537043",
   "metadata": {},
   "source": [
    "## HIPT_4K Attention Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8907f80-a064-4e42-b11e-c96f9a7f5825",
   "metadata": {},
   "source": [
    "##### Code for producing attention results (for [256 x 256], [4096 x 4096], and hierarchical [4096 x 4096]) can be run (as-is) below. There are several ways these results can be run:\n",
    "\n",
    "hipt_4k.py Class (Preferred): This class blends inference and heatmap creation in a seamless and more object-oriented manner, and is where I am focusing my future code development around.\n",
    "Helper Functions in hipt_heatmap_utils.py (Soon-to-be-deprecated): Heatmap creation was originally written as helper functions. May be more useful and easier from research perspective.\n",
    "Please use whatever is most helpful for your use case :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8df08-15ef-4275-8660-7dd2bd96ec28",
   "metadata": {},
   "source": [
    "## 256 x 256 Demo (Saving Attention Maps Individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1770a560-73d4-4912-a0ae-d190b868a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch = Image.open('./image_demo/image_256.png')\n",
    "# output_dir = './attention_demo/256_output_indiv/'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# create_patch_heatmaps_indiv(patch=patch, model256=model256, \n",
    "#                             output_dir=output_dir, fname='patch',\n",
    "#                             cmap=light_jet, device256=device256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0de90-5bcc-4d35-814a-3194aaca74aa",
   "metadata": {},
   "source": [
    "## 256 x 256 Demo (Concatenating + Saving Attention Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40f3710-2b93-426d-9d53-c0e30ad538ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch = Image.open('./image_demo/image_256.png')\n",
    "# output_dir = './attention_demo/256_output_concat/'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# create_patch_heatmaps_concat(patch=patch, model256=model256, \n",
    "#                             output_dir=output_dir, fname='patch',\n",
    "#                             cmap=light_jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e56dd-a036-4f1b-84cb-9efadfafbd99",
   "metadata": {},
   "source": [
    "## 4096 x 4096 Demo (Saving Attention Maps Individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41129766-0c87-43a3-b643-c0f632dbcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = Image.open('./image_demo/image_4k.png')\n",
    "output_dir = './attention_demo/4k_output_indiv/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "create_hierarchical_heatmaps_indiv(region, model256, model4k, \n",
    "                                   output_dir, fname='region', \n",
    "                                   scale=2, threshold=0.5, cmap=light_jet, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40431b84-9141-4903-9617-3357bc34b199",
   "metadata": {},
   "source": [
    "## 4096 x 4096 Demo (Concatenating + Saving Attention Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1428fc25-b3fe-4102-8a38-d3faedc3e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = Image.open('./image_demo/image_4k.png')\n",
    "# output_dir = './attention_demo/4k_output_concat/'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# create_hierarchical_heatmaps_concat(region, model256, model4k, \n",
    "#                                    output_dir, fname='region', \n",
    "#                                    scale=2, cmap=light_jet, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b14ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a2edfd9441314e81940a8db140f035b2781395eeff1de1e2eee74d8a060d612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
